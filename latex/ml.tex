\subsection{Classification Models}

    To test the contribution of the summary features, we used a binary classifier with
    $y_i$ as the outcome variable. We trained four models, which used the following sets of features:

    \begin{description}
        \item[baseline 1:] Predict turn transition based only on the current dialog act label.
        \item[baseline 2:] Predict turn transition based on the labels of the current and previous dialog acts.
        \item[summary model:] Predict turn transition using just the summary features.
        \item[full model:] Predict turn transition using the summary features and the current and previous dialog acts.
    \end{description}


We used random forests to build the binary classifiers $(N=200)$ \cite{Breiman01randomforests}. Random forests build an ensemble of decision trees during training, and during testing, each decision tree votes on the outcome.  Like decision trees, they can account for interactions between variables, such as making greater use of the summary features when the current speech act is not a question.  Random forests though are not as sensitive to overfitting and data fragmentation.

We performed 10 fold-labeled cross validations.  We made sure that each conversation was entirely in a single fold. This way, each dialogue was entirely used for training or testing, but never for both at the same time.

\subsection{Metrics}
To evaluate our hypothesis, we use the trained model to perform prediction of the test data. We than compare the predictions against the truth values. The results are recorded in a confusion matrix as shown in table \ref{tab:mapping}.
Each row in the matrix represent the actual class and each column represent the predicted class.
The cell in the matrix are compute as follow :
%
\begin{table}[ht!]
\begin{center}
\begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
  \multirow{10}{*}{\rotatebox{90}{\parbox{2cm}{\bfseries Caused turn change}}} &
    & \multicolumn{2}{c}{\bfseries Predicted to cause turn change} & \\
  & & \bfseries yes & \bfseries no & \\
  & yes & \MyBox{True}{Positive} & \MyBox{False}{Negative} \\[2.4em]
  & no & \MyBox{False}{Positive} & \MyBox{True}{Negative}  \\
\end{tabular}
\end{center}\vspace{-0.5em}
\caption{Confusion Matrix}
\label{tab:mapping}
\end{table}


\begin{enumerate}
  \item True Positive - Count of the dialog acts which were predicted to cause a turn change and led to turn change.
  \item False Positive-  Count of the dialog acts which were predicted to cause a turn change but in fact did not cause one.
  \item True Negative - Count of dialog acts which were predicted to not cause a turn change and did not cause turn change.
  \item False Negative - Count of dialog acts which were predicted to not cause a turn change but in fact caused one.
\end{enumerate}

Based on the confusion matrix, we compute the following metrics for each model:

\begin{equation}
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}
The accuracy measure how many dialog acts were classified correctly out of the total dialog acts.

\begin{equation}
Precision = \frac{TP }{TP + FP}
\end{equation}
The precision metric measure how accurate is the classifier for the the dialog acts that caused turn change. I.e. how many dialog acts that were classified as causing a turn change, did in fact led to a turn change.

\begin{equation}
Recall = \frac{TP }{TP + FN}
\end{equation}
The recall metric measure how many dialog act that predicted turn change were detected by the classifier.

\begin{equation}
F1 = \frac{2 \cdot Precision\cdot Recall}{Precision+ Recall}
\end{equation}
In order to have one measurement which encompass both recall and precision, we compute F1 which is the
harmonic mean of recall and precision.

Note that there is a trade off between recall and precision. I.e. if we want to increase recall we will reduce precision. To measure this tradeoff, we will use ROC (Receiver Operating Characteristic) curve.
The ROC curve measure the true positive rate (TPR) or recall, against the false positive rate (FPR).
When comparing different ROC curves, we measure the area under the curve (AUC).

