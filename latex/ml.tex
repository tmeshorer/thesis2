\section{Classification Models}

    To test the contribution of the summary features, we used two types of binary classifiers with
    $y_i$ as the outcome variable. We trained four models for each classifier, which used the following sets of features:

    \begin{description}
        \item[baseline 1:] Predict turn transition based only on the current dialog act label.
        \item[baseline 2:] Predict turn transition based on the labels of the current and previous dialog acts.
        \item[summary model:] Predict turn transition using just the summary features.
        \item[full model:] Predict turn transition using the summary features and the current and previous dialog acts.
    \end{description}


For the first classifier, we used random forests to build the binary classifiers $(N=200)$ \cite{Breiman01randomforests}. Random forests build an ensemble of decision trees during training, and during testing, each decision tree votes on the outcome.  Like decision trees, they can account for interactions between variables, such as making greater use of the summary features when the current speech act is not a question.  Random forests though are not as sensitive to overfitting and data fragmentation.


For the second classifier we used gradient boosting \cite{friedman2001greedy}. The gradient boosting classifier uses a combination of a loss function and weak classifiers to create an ensemble of weak classifiers, which perform a majority vote. At each step of the classifier boosting, the weight of hard to classify examples is boosted, such that new weak classifiers are trained on the hard to classify data.

We performed 10 fold-labeled cross validations.  We made sure that each conversation was entirely in a single fold. This way, each dialogue was entirely used for training or testing, but never for both at the same time.

\section{Metrics}
To evaluate our hypothesis, we use the trained models to predict the turn taking behavior in our data. We than compare the predictions against the truth values. The results are shown in the confusion matrix shown in Table \ref{tab:mapping}.
Each row in the matrix represent the true class and each column represent the predicted class.
The cells in the matrix are computed as follows:
%
\begin{table}[ht!]
\begin{center}
\begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
  \multirow{10}{*}{\rotatebox{90}{\parbox{2cm}{\bfseries Caused turn change}}} &
    & \multicolumn{2}{c}{\bfseries Predicted to cause turn change} & \\
  & & \bfseries yes & \bfseries no & \\
  & yes & \MyBox{True}{Positive} & \MyBox{False}{Negative} \\[2.4em]
  & no & \MyBox{False}{Positive} & \MyBox{True}{Negative}  \\
\end{tabular}
\end{center}\vspace{-0.5em}
\caption{Confusion Matrix}
\label{tab:mapping}
\end{table}


\begin{enumerate}
  \item True Positive: Count of the dialog acts that were predicted to cause a turn change and that did lead to turn change.
  \item False Positive: Count of the dialog acts that were predicted to cause a turn change but in fact did not cause one.
  \item True Negative: Count of dialog acts that were predicted to not cause a turn change and did not cause turn change.
  \item False Negative: Count of dialog acts that were predicted to not cause a turn change but in fact caused one.
\end{enumerate}

Based on the confusion matrix, we compute the following metrics for each model:

\begin{equation}
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}
Accuracy measures how many dialog acts are classified correctly out of the total dialog acts.

\begin{equation}
Precision = \frac{TP }{TP + FP}
\end{equation}
Precision measures how accurate the classifier is for the dialog acts that caused turn change; i.e., how many dialog acts that were classified as causing a turn change, did in fact led to a turn change.


\begin{equation}
Recall = \frac{TP }{TP + FN}
\end{equation}
Recall measures how many dialog acts that predicted turn change were detected by the classifier.

\begin{equation}
F1 = \frac{2 \cdot Precision\cdot Recall}{Precision+ Recall}
\end{equation}
In order to have one measurement which encompass both recall and precision, we compute F1, which is the
harmonic mean of recall and precision.

Note that there is a trade off between recall and precision: if we want to increase recall we will reduce precision. To measure this tradeoff, we will use ROC (Receiver Operating Characteristic) curves.
The ROC curve measures the true positive rate or recall, against the false positive rate (FPR).
When comparing different ROC curves, we measure the area under the curve (AUC).

